### 08.13.

딥러닝의 기본 구조와 핵심 개념을 이해하기 위해 퍼셉트론부터 시작했다. 퍼셉트론은 인공신경망의 가장 기초가 되는 단위로, 입력값에 가중치를 곱하고 활성함수를 통해 출력을 결정하는 모델이다. 이 과정을 통해 딥러닝이 데이터를 어떻게 처리하는지에 대한 직관을 얻을 수 있었다.

또한 머신러닝 학습 방법의 세 가지 큰 축인 지도학습, 비지도학습, 강화학습(Reinforcement Learning)의 개념도 학습했다.
<br>
지도학습: 입력과 정답(레이블)이 주어져 모델이 이를 학습하는 방식. (예: 이미지 분류)
<br>
비지도학습: 정답 없이 데이터의 패턴이나 군집을 찾는 방식. (예: 클러스터링, 차원 축소)
<br>
강화학습: 에이전트가 환경과 상호작용하며 보상 신호를 기반으로 학습하는 방식. (예: 게임 플레이, 로봇 제어)
<br><br>
마지막으로 딥러닝에서 자주 쓰이는 활성함수(Activation Function)에 대해 정리했다. 주요 함수와 특징은 다음과 같다:
<br>
Sigmoid 함수: 출력값을 0~1로 제한. 확률적 해석이 가능하지만, 기울기 소실(Vanishing Gradient) 문제 발생.
<br>
Tanh 함수: 출력 범위를 -1~1로 확장, Sigmoid보다 중심화된 값 제공. 그러나 여전히 기울기 소실 문제 존재.
<br>
Leaky ReLU: ReLU의 단점을 보완하기 위해 음수 영역에 작은 기울기 부여.
<br>
Softmax: 주로 출력층에서 사용되며, 확률 분포를 나타내는 데 유용.
<br><br>
퍼셉트론이 기초 단위라서 단순해 보이지만, 실제로는 딥러닝의 출발점이라는 점이 흥미로웠다. 활성함수 선택이 모델 성능에 미치는 영향이 크다는 점도 새로 알게 되었다. 다음에는 다층 퍼셉트론(MLP) 구조와 Backpropagation에 대해 학습할 계획이다.
